{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import shutil \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "\n",
    "\n",
    "mean = 0\n",
    "std=1\n",
    "\n",
    "img_size=32\n",
    "torch.manual_seed(10)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('../../datasets/mnist', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                                torchvision.transforms.Resize([img_size, img_size]),\n",
    "                                torchvision.transforms.ToTensor(),\n",
    "                                torchvision.transforms.Normalize(\n",
    "                                 (mean,), (std,))\n",
    "                             ])),\n",
    "  batch_size=60000, shuffle=True)\n",
    "\n",
    "val_test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../../datasets/mnist', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                                torchvision.transforms.Resize([img_size, img_size]),\n",
    "                                torchvision.transforms.ToTensor(),\n",
    "                                torchvision.transforms.Normalize(\n",
    "                                 (mean,), (std,))\n",
    "                             ])),\n",
    "  batch_size=10000, shuffle=True, drop_last= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ =next(iter(train_loader))\n",
    "val_test_data, _ =next(iter(val_test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total samples: 10000\n",
      "unique : (10000, 1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "## CORRECT WAY TO SHUFFLE WITHOUT REPEATS !!!\n",
    "#val_test_data = val_test_data[np.random.choice(np.arange(len(val_test_data)), len(val_test_data), replace=False)] ## SHUFFLE !!!\n",
    "u, c = np.unique(val_test_data, return_index=True, axis=0)\n",
    "print(f'number of total samples: {len(val_test_data)}')\n",
    "print(f'unique : {u.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_grids(data, images_per_grid, nrow_per_grid, start_idx, save_dir, type_, remove_existing_dir= False):\n",
    "    if remove_existing_dir:\n",
    "        try:shutil.rmtree(f\"{save_dir}/{type_}\")\n",
    "        except:pass\n",
    "    \n",
    "    try:os.mkdir(f\"{save_dir}/{type_}\")\n",
    "    except:pass\n",
    "    \n",
    "    np.random.seed(500)\n",
    "    data = data[np.random.choice(np.arange(len(data)), len(data), replace=False)] ## SHUFFLE !!!\n",
    "    \n",
    "    ## CHECK DUPLICATES\n",
    "    uniques, c = np.unique(data, return_index=True, axis=0)\n",
    "    assert uniques.shape==data.shape, f'DATASET HAVE DUPLICATED SAMPLES !!! --> uniques.shape ({uniques.shape}) != data.shape ({data.shape})'\n",
    "    print(f'NO DUPLICATES in the dataset ...  --> uniques.shape ({uniques.shape}) == data.shape ({data.shape})')\n",
    "    ## CHECK DUPLICATES\n",
    "\n",
    "    for i in range(0, len(data), images_per_grid):\n",
    "        grid = torchvision.utils.make_grid(data[i:i+images_per_grid], padding=0, nrow= nrow_per_grid).permute(1,2,0).cpu().numpy()\n",
    "        \n",
    "        img_idx= start_idx + i//images_per_grid+1\n",
    "        img_save_dir = f\"{save_dir}/{type_}/{img_idx}.jpg\"\n",
    "        plt.imsave(img_save_dir, grid)\n",
    "        if i==0: print(\"saving ...  (first grid image): \", img_save_dir)\n",
    "    print(f\"saving ...  (last grid image): {img_save_dir}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cp -r \"/n/home06/udithhaputhanthri/project_udith/datasets/mnistgrid_mnistsize(32)_imgsize(320)_v2\"  \"/n/holyscratch01/wadduwage_lab/uom_Udith/datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset directory ::  /n/home06/udithhaputhanthri/project_udith/datasets/mnistgrid_mnistsize(32)_imgsize(640)\n"
     ]
    }
   ],
   "source": [
    "images_per_grid= 100*4\n",
    "nrow_per_grid= int(images_per_grid**0.5)\n",
    "\n",
    "repeat_train= 100*4\n",
    "repeat_valtest= 84\n",
    "\n",
    "save_dir = f\"/n/home06/udithhaputhanthri/project_udith/datasets/mnistgrid_mnistsize({img_size})_imgsize({img_size*nrow_per_grid})\"\n",
    "\n",
    "print('dataset directory :: ', save_dir)\n",
    "\n",
    "try:os.mkdir(save_dir)\n",
    "except:print(f'available directory :: {save_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(repeat_train):\n",
    "    if idx==0:remove_existing_dir= True\n",
    "    else:remove_existing_dir= False\n",
    "        \n",
    "    save_grids(train_data, images_per_grid, nrow_per_grid, idx*(len(train_data)//images_per_grid), save_dir, 'train', remove_existing_dir= remove_existing_dir) # save 600 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(repeat_valtest):#range(20*4):\n",
    "    if idx==0:remove_existing_dir= True\n",
    "    else:remove_existing_dir= False\n",
    "    \n",
    "    val_test_data = val_test_data[np.random.choice(np.arange(len(val_test_data)), len(val_test_data), replace=False)] ## SHUFFLE WITHOUT REPEATS!!!\n",
    "    save_grids(val_test_data[:5000], images_per_grid, nrow_per_grid, idx*(5000//images_per_grid), save_dir, 'val', remove_existing_dir= remove_existing_dir) # save 50 images\n",
    "    save_grids(val_test_data[5000:], images_per_grid, nrow_per_grid, idx*(5000//images_per_grid), save_dir, 'test', remove_existing_dir= remove_existing_dir) # save 50 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepTFM",
   "language": "python",
   "name": "deeptfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
