{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3KJr0JZ-8iys"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "\n",
    "def get_yt_map_idx_grid(img_size, scale_factor):\n",
    "    yt_img_size= img_size//scale_factor\n",
    "    yt_idx_grid= torch.arange(yt_img_size**2).reshape(yt_img_size, yt_img_size)\n",
    "    yt_map_idx_grid_flatten= torch.tile(yt_idx_grid, (scale_factor,scale_factor,1,1)).permute(2, 0, 3, 1).reshape(img_size, img_size).flatten()\n",
    "\n",
    "    return yt_map_idx_grid_flatten\n",
    "\n",
    "def convert_Ht2A(Ht, lambda_scale_factor):\n",
    "    \"\"\"\n",
    "    Convert Ht to sparse matrix A/ forward model matrix including downsampling\n",
    "\n",
    "    Ht.shape: [1, T, img_size, img_size]\n",
    "    X.shape: [n_imgs, 1, img_size, img_size]\n",
    "    \"\"\"\n",
    "    img_size= Ht.shape[2]\n",
    "    T= Ht.shape[1]\n",
    "    scale_factor= 2**(lambda_scale_factor-1)\n",
    "    yt_img_size= img_size//scale_factor\n",
    "\n",
    "    Ht_flatten= Ht.reshape(-1) #shape: (T*img_size*img_size, )\n",
    "    A= torch.zeros(T, yt_img_size**2, img_size**2).float() #shape: (T, yt_img_size**2, img_size**2)  ## CAUTION: MEMORY HUNGRY- 1 !!!\n",
    "\n",
    "    yt_map_idx_grid_flatten= get_yt_map_idx_grid(img_size, scale_factor) #shape: (img_size, img_size)\n",
    "\n",
    "    depth= torch.tile(torch.arange(T).reshape(1, -1), (img_size*img_size, 1)).T.reshape(-1)\n",
    "    column= torch.tile(torch.arange(img_size*img_size), (T,)) \n",
    "    row= torch.tile(yt_map_idx_grid_flatten[np.arange(img_size*img_size)], (T,))\n",
    "    \n",
    "    A= A.index_put(indices=[depth, row, column], values=Ht_flatten.float()).unsqueeze(dim=0) ## CAUTION: MEMORY HUNGRY- 3 !!!\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igtWteG5SFSp",
    "outputId": "caac674d-787a-4016-b690-7f4f5c135df8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixA shape: (8, 256, 16384)\n",
      "tot. memory requirement for MatrixA : 0.134217728 GB !!! \n"
     ]
    }
   ],
   "source": [
    "img_size= 128\n",
    "lambda_scale_factor= 4\n",
    "T=8\n",
    "\n",
    "scale_factor= 2**(lambda_scale_factor-1)\n",
    "yt_img_size= img_size//scale_factor\n",
    "\n",
    "print(f'MatrixA shape: {(T, yt_img_size**2, img_size**2)}')\n",
    "print(f'tot. memory requirement for MatrixA : {(T*yt_img_size**2*img_size**2)*4*1e-9} GB !!! ')\n",
    "\n",
    "\n",
    "Ht= torch.randint(0, 10, (1, T, img_size, img_size))\n",
    "A= convert_Ht2A(Ht, lambda_scale_factor)\n",
    "X= torch.randint(0, 150, (32, 1, img_size, img_size)).float()\n",
    "#X= torch.randn(32, 1, img_size, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaK34h4NXlHM",
    "outputId": "3c05fe8b-bd54-4887-cf41-ea05476a8f51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is identical : True\n",
      "is close (how much) : 1.0\n"
     ]
    }
   ],
   "source": [
    "upscale_method1= (F.avg_pool2d((Ht*X), (scale_factor, scale_factor))*(scale_factor**2))\n",
    "upscale_method2 = (A @ X.float().flatten(start_dim= 2).unsqueeze(dim=3)).reshape(-1, T, yt_img_size, yt_img_size)\n",
    "\n",
    "print(f'is identical : {(upscale_method1== upscale_method2).all()}')\n",
    "print(f'is close (how much) : {torch.isclose(upscale_method1, upscale_method2).float().mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a53pwah92z9p"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-C-brXRy36LE"
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGNyOBfr21rK"
   },
   "source": [
    "## 1. Upsampling initialization (only) using A.transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JsqoeY9zon3",
    "outputId": "5aba2ef2-ad92-4709-a9d5-c968f9751faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial, upscaled shapes : torch.Size([32, 8, 16, 16]) | torch.Size([32, 8, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "yt= torch.randint(0, 150, (32, T, yt_img_size, yt_img_size)).float()\n",
    "batch_size= yt.shape[0]\n",
    "\n",
    "Ht= torch.randn(1, T, img_size, img_size)\n",
    "A= convert_Ht2A(Ht, lambda_scale_factor)\n",
    "A_transpose= nn.Parameter(A.permute(0,1,3,2), requires_grad= True) # Get transpose for upsampling (Approx for inverse(A))\n",
    "yt_upscaled = (A_transpose @ yt.flatten(start_dim= 2).unsqueeze(dim=3)).reshape(-1, T, img_size, img_size)\n",
    "print(f'initial, upscaled shapes : {yt.shape} | {yt_upscaled.shape}')\n",
    "\n",
    "yt_upscaled.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pagAnv8K28dl"
   },
   "source": [
    "## 2. Learning Ht through Upsampling using A.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_u8_ST60qcM",
    "outputId": "04482a93-6f39-4f90-a9de-9df134c2ceef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial, upscaled shapes : torch.Size([32, 8, 16, 16]) | torch.Size([32, 8, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "yt= torch.randint(0, 150, (32, T, yt_img_size, yt_img_size)).float()\n",
    "batch_size= yt.shape[0]\n",
    "\n",
    "Ht= nn.Parameter(torch.randn(1, T, img_size, img_size), requires_grad= True)\n",
    "A= convert_Ht2A(Ht, lambda_scale_factor)\n",
    "A_transpose= A.permute(0,1,3,2) # Get transpose for upsampling (Approx for inverse(A))\n",
    "yt_upscaled = (A_transpose @ yt.flatten(start_dim= 2).unsqueeze(dim=3)).reshape(-1, T, img_size, img_size)\n",
    "print(f'initial, upscaled shapes : {yt.shape} | {yt_upscaled.shape}')\n",
    "\n",
    "yt_upscaled.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "762HD8Vw2WsM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ht2A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
